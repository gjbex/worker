# directory and files to be created by worker
  # temp dir template
  dir_tmpl = .workerXXXX

  # PBS script name and shebang (don't forget to escape shebang!)
  pbs_file = worker.pbs
  pbs_shebang = \#!/bin/bash -l
  pbs_prefix = \#SBATCH

  # extensions for files to be created in the working directory
  # arrayid files
  arrayid_ext = .array
  # generated batch file
  batch_ext   = .worker
  # submit script
  run_ext     = .run
  # preprocessed batch, only generated in verbose mode
  prepr_ext   = .pp

# file name templates so that files are renamed consistently
  # log name
  default_log = ${SLURM_JOB_NAME}.log${WORKER_JOBID}
  # generated batch name
  default_sh  = ${SLURM_JOB_NAME}.sh${WORKER_JOBID}
  # generated sqlite3 name for log analysis
  default_sql = ${SLURM_JOB_NAME}.sqlite3${WORKER_JOBID}
  # submit script name
  default_run = ${SLURM_JOB_NAME}.run${WORKER_JOBID} 
  # prolog name
  default_pro = ${SLURM_JOB_NAME}.pro${WORKER_JOBID}
  # epilog name
  default_epi = ${SLURM_JOB_NAME}.epi${WORKER_JOBID} 
  # PBS name
  default_pbs = ${SLURM_JOB_NAME}.pbs${WORKER_JOBID}
  # host name
  default_host = ${SLURM_JOB_NAME}.host${WORKER_JOBID}

# files to be found in the worker directory
  # PBS template to complete
  pbs_tmpl_dir = lib/tt
  pbs_tmpl_ext = .tt
  # worker header file to parse for separator
  worker_hdr = conf/worker.h
  # worker executable
  worker = bin/worker

# hardware properties
  cores_per_node = 20

# job submission command
  qsub = sbatch

# notification mail address
  email = hpcinfo@kuleuven.be

# MPI related configuration
  unload_modules = intel
  module_path = /apps/leuven/ivrybridge/2018a/modules/all
  mpi_module = intel/2018a
  mpirun = mpirun
  mpirun_options =

# sleep time in microseconds between MPI_Test calls in master
  default_sleep = 10000
